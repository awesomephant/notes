---
layout: post
title: "Designing a Feminist Alexa"
date: 2018-10-16 18:00:00
teacher: "Various"
unit: 'UAL Creative Computing Institute'
abstract: ""
website: ""
thumb: ""
---

This is the first public event of [the creative computing institute](https://www.arts.ac.uk/creative-computing-institute)

- Inclusion
- Entrepeneurship (Yuch)

## Feminist Internet
This comes out of ual futures. The mission is to advance equality on the internet.

## Gendering of personal assistants (i.e Alexa, Google, Siri, Cortana)

The internet of things is a thing.

Why is every slide a gif

Tese personal assistants have two components:
- Networked objects, which are a part of everyday life 
- Intelligent algorithms which are making decisions all around us

If we fail to make ethical and inclusive artificial intelligence, we risk...

Tae (the Microsoft chatbot)
> The more yyou chat with Tay the smarter she gets, so the experience can be more personalized to you

She turned into a nazi blah blah we know the story

[Microsoft Zo is the latest iteration](https://www.zo.ai/) of this.

People working on AI ethics:

- Ada Lvelace institute
- centre for data ethics and innovation
- women in ai
- ai now institute

> it's hard to think that, goven the chouce, most people would object...
Ruth Abrahams

> By encouraging consumers to understand the objects...
Jaqline Feldman

Tech companies say to this: It's just what the market wants. AIs are designed as women, respond to abusive language in 

Leah Fessler, Quartz Magazine (2017): Harrasment of AI assistants. There's been some pushback to that (and some changes), but that's not as good as intervening at the design/development stage.

## Panel

- Feminist AI researcher **Josie Young**
- Founder Acorn Aspirations, Teens in AI **Elena Sinel**
- Co-Founder, Head Creative Technologist, [Comuzi](https://comuzi.xyz/) **Alex Fefegha** (Comuzi is an ad agency)

## Josie Young on Feminist Chatbots
How do we interrogate how we design chatbots. Chatbots are probably (?) the main interface we have with AI. If you call up a government agency, talk to your laptop, Facebook etc., you're taling to a chatbot. Biases in chatbots *seep back* into society in all kinds of ways.

Should chatbots have a gender? Nope.
- When a gender is attached to chatbots, it's usually done in a stereotypical way.  Female bots are assistants, navigation, male bots gove law advice etc.
- When we gender bots, it prompts negative reactions from people interacting with it. When Cortana was first deployed, the most common question was wether she had a boyfriend (citation needed).
- This tech reaches a lot of people, so there's a huge amount of responsibility
- Constrains design of robots - you've limited how that bot can express itself, connect with others, what it can do. 

### Feminist research design process

Uzbekistan isn't a great place.

[Teens in AI](https://teensinai.com/) does bootcamps, hackathons etc.

<iframe width="560" height="315" src="https://www.youtube.com/embed/61zXhXcf6Vw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

ehhhhh.

## Alex Fefegha: Algorithms and the Life of Brisha Borden

This is his CSM MA Thesis.

AI as
> the art of how to make computers do thins at which humans are currently better at

Brisha Borden was a Florida teen. She got arrested for stealing a bike - the judge in the case was using a re-offending score software. Of course the thing's racist.

This is detailed in a [2016 ProPublica Investigation](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

The offenders in Florida would get a survey where they ask questions which are essentially designed to filter out poor people. Of course this plays into disproportionate sentencing of black people in the US>

Responses to the ProPublica piece: 

- [False Positives, False Negatives, and False Analyses: A Rejoinder to "Machine Bias: There's Software Used Across the Country to Predict Future Criminals. And It's Biased Against Blacks.](http://www.uscourts.gov/federal-probation-journal/2016/09/false-positives-false-negatives-and-false-analyses-rejoinder)
- [COMPAS Risk Scales: Demonstrating](http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf)
- [the accuracy, fairness, and limits of predicting recidivism](http://advances.sciencemag.org/content/4/1/eaao5580)

- All of this software is written by private companies, the algorithms aren't transparent. 
- Based on group data
- Question of national v local data 

This is all based on US data and reporting, how does this play in a UK context. Ran workshops etc. with Comuzi.

### Conclusions

- AI is jut making bad decisions faster
- Fairness is a subjective thing, hard to do with maths
- accountability of algorithms and data is hard
- more conversations on bias are needed
- teams need to be diverse

Philip Alston:

> It is extremely important for an audiennce interested in AI to recognize that when we take a social welfare system and ... put on top of it ways of making it more efficient

[AI Cheatsheet](https://aicheatsheet.comuzi.xyz/)

## Questions

### How do we balance changing tech vs changing society

- **Young**: They need to be intertwined. If we bring in social scientists, philosophers together with people building the AI this can happen.
- **Fefegha**: Tech is an extension of ourselves. Humanity isn't nice, but if we're going to introduce AI systems we need to recognize our on flaws. We need to have conversations on how data collection is biased etc.
- **Young**: Governments and companies are setting the classification, who goes in a residual category.

### Do we need standards / global regulation for AI

- **Fefegha**: Part of [IEEE](https://www.ieee.org/), which is trying to develop standards for building ethical AI. We can build these frameworks, but how are they enforced, measured, regulated. Of course industry is trying to avoid regulation.
- **Young**: This all needs to happen at different levels / layers: The teams building the software, people using it, governments regulating it etc. Also: Innoation used to happen in universities (which have all this ethics infrastructure) - now that happens inside companies, which don't have any of those frameworks.

Calvert makes her point about Artificial intelligence / partial intelligence

**Fefegha**: I stay away from that conversation and focus on real-world issues that affect people now (i.e sentencing)
**Young**: A more opimistic of the future, where AI creates, works together. Her (2016) as opposed to Ex Machina

